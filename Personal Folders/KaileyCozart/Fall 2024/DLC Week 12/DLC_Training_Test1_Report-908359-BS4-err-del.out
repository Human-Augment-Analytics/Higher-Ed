---------------------------------------
Begin Slurm Prolog: Nov-04-2024 01:32:25
Job ID:    908359
User ID:   kcozart6
Account:   coc
Job name:  DLC_model_training
Partition: ice-gpu
---------------------------------------

CondaError: Run 'conda init' before 'conda activate'

Training with configuration:
data:
  colormode: RGB
  inference:
    normalize_images: True
  train:
    affine:
      p: 0.5
      rotation: 30
      scaling: [1.0, 1.0]
      translation: 0
    collate:
      type: ResizeFromDataSizeCollate
      min_scale: 0.4
      max_scale: 1.0
      min_short_side: 128
      max_short_side: 1152
      multiple_of: 32
      to_square: False
    covering: False
    gaussian_noise: 12.75
    hist_eq: False
    motion_blur: False
    normalize_images: True
device: auto
metadata:
  project_path: /storage/ice1/0/3/kcozart6/dlc_model-student-2023-07-26
  pose_config_path: /storage/ice1/0/3/kcozart6/dlc_model-student-2023-07-26/dlc-models-pytorch/iteration-0/dlc_modelJul26-trainset95shuffle1/train/pose_cfg.yaml
  bodyparts: ['nose', 'lefteye', 'righteye', 'spine1', 'spine2', 'spine3', 'backfin', 'leftfin', 'rightfin']
  unique_bodyparts: []
  individuals: ['fish1', 'fish2', 'fish3', 'fish4', 'fish5', 'fish6', 'fish7', 'fish8', 'fish9', 'fish10']
  with_identity: False
method: bu
model:
  backbone:
    type: ResNet
    model_name: resnet50_gn
    output_stride: 16
    freeze_bn_stats: True
    freeze_bn_weights: False
  backbone_output_channels: 2048
  heads:
    bodypart:
      type: DLCRNetHead
      predictor:
        type: PartAffinityFieldPredictor
        num_animals: 10
        num_multibodyparts: 9
        num_uniquebodyparts: 0
        nms_radius: 5
        sigma: 1.0
        locref_stdev: 7.2801
        min_affinity: 0.05
        graph: [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 5], [4, 6], [4, 7], [4, 8], [5, 6], [5, 7], [5, 8], [6, 7], [6, 8], [7, 8]]
        edges_to_keep: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
        apply_sigmoid: True
        clip_scores: False
      target_generator:
        type: SequentialGenerator
        generators: [{'type': 'HeatmapPlateauGenerator', 'num_heatmaps': 9, 'pos_dist_thresh': 17, 'heatmap_mode': 'KEYPOINT', 'gradient_masking': False, 'generate_locref': True, 'locref_std': 7.2801}, {'type': 'PartAffinityFieldGenerator', 'graph': [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 5], [4, 6], [4, 7], [4, 8], [5, 6], [5, 7], [5, 8], [6, 7], [6, 8], [7, 8]], 'width': 20}]
      criterion:
        heatmap:
          type: WeightedBCECriterion
          weight: 1.0
        locref:
          type: WeightedHuberCriterion
          weight: 0.05
        paf:
          type: WeightedHuberCriterion
          weight: 0.1
      heatmap_config:
        channels: [2048, 9]
        kernel_size: [3]
        strides: [2]
      locref_config:
        channels: [2048, 18]
        kernel_size: [3]
        strides: [2]
      paf_config:
        channels: [2048, 72]
        kernel_size: [3]
        strides: [2]
      num_stages: 5
net_type: resnet_50
runner:
  type: PoseTrainingRunner
  gpus: None
  key_metric: test.mAP
  key_metric_asc: True
  eval_interval: 300
  optimizer:
    type: AdamW
    params:
      lr: 0.0001
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [[1e-05], [1e-06]]
      milestones: [160, 190]
  snapshots:
    max_snapshots: 7
    save_epochs: 25
    save_optimizer_state: False
train_settings:
  batch_size: 4
  dataloader_workers: 0
  dataloader_pin_memory: False
  display_iters: 500
  epochs: 140
  seed: 42
Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)
[timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Data Transforms:
  Training:   Compose([
  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (1.0, 1.0), 'y': (1.0, 1.0)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),
  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
  Validation: Compose([
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
Using custom collate function: {'type': 'ResizeFromDataSizeCollate', 'min_scale': 0.4, 'max_scale': 1.0, 'min_short_side': 128, 'max_short_side': 1152, 'multiple_of': 32, 'to_square': False}

Note: According to your model configuration, you're training with batch size 1 and/or ``freeze_bn_stats=false``. This is not an optimal setting if you have powerful GPUs.
This is good for small batch sizes (e.g., when training on a CPU), where you should keep ``freeze_bn_stats=true``.
If you're using a GPU to train, you can obtain faster performance by setting a larger batch size (the biggest power of 2 where you don't geta CUDA out-of-memory error, such as 8, 16, 32 or 64 depending on the model, size of your images, and GPU memory) and ``freeze_bn_stats=false`` for the backbone of your model. 
This also allows you to increase the learning rate (empirically you can scale the learning rate by sqrt(batch_size) times).

Using 1472 images and 78 for testing

Starting pose model training...
--------------------------------------------------
Epoch 1/140 (lr=0.0001), train loss 0.00961
Epoch 2/140 (lr=0.0001), train loss 0.00545
Epoch 3/140 (lr=0.0001), train loss 0.00397
Epoch 4/140 (lr=0.0001), train loss 0.00422
Epoch 5/140 (lr=0.0001), train loss 0.00418
Epoch 6/140 (lr=0.0001), train loss 0.00353
Epoch 7/140 (lr=0.0001), train loss 0.00384
Epoch 8/140 (lr=0.0001), train loss 0.00451
Epoch 9/140 (lr=0.0001), train loss 0.00475
Epoch 10/140 (lr=0.0001), train loss 0.00284
Epoch 11/140 (lr=0.0001), train loss 0.00452
Epoch 12/140 (lr=0.0001), train loss 0.00293
Epoch 13/140 (lr=0.0001), train loss 0.00307
Epoch 14/140 (lr=0.0001), train loss 0.00242
Epoch 15/140 (lr=0.0001), train loss 0.00229
Epoch 16/140 (lr=0.0001), train loss 0.00247
Epoch 17/140 (lr=0.0001), train loss 0.00284
Epoch 18/140 (lr=0.0001), train loss 0.00280
Epoch 19/140 (lr=0.0001), train loss 0.00250
Epoch 20/140 (lr=0.0001), train loss 0.00342
Epoch 21/140 (lr=0.0001), train loss 0.00282
Epoch 22/140 (lr=0.0001), train loss 0.00219
Epoch 23/140 (lr=0.0001), train loss 0.00212
Epoch 24/140 (lr=0.0001), train loss 0.00229
Epoch 25/140 (lr=0.0001), train loss 0.00219
Epoch 26/140 (lr=0.0001), train loss 0.00208
Epoch 27/140 (lr=0.0001), train loss 0.00236
Epoch 28/140 (lr=0.0001), train loss 0.00204
Epoch 29/140 (lr=0.0001), train loss 0.00193
Epoch 30/140 (lr=0.0001), train loss 0.00195
Epoch 31/140 (lr=0.0001), train loss 0.00195
Epoch 32/140 (lr=0.0001), train loss 0.00204
Epoch 33/140 (lr=0.0001), train loss 0.00196
Epoch 34/140 (lr=0.0001), train loss 0.00185
Epoch 35/140 (lr=0.0001), train loss 0.00179
Epoch 36/140 (lr=0.0001), train loss 0.00183
Epoch 37/140 (lr=0.0001), train loss 0.00186
Epoch 38/140 (lr=0.0001), train loss 0.00167
Epoch 39/140 (lr=0.0001), train loss 0.00200
Epoch 40/140 (lr=0.0001), train loss 0.00164
Epoch 41/140 (lr=0.0001), train loss 0.00176
Epoch 42/140 (lr=0.0001), train loss 0.00159
Epoch 43/140 (lr=0.0001), train loss 0.00232
Epoch 44/140 (lr=0.0001), train loss 0.00171
Epoch 45/140 (lr=0.0001), train loss 0.00166
Epoch 46/140 (lr=0.0001), train loss 0.00164
Epoch 47/140 (lr=0.0001), train loss 0.00157
Epoch 48/140 (lr=0.0001), train loss 0.00159
Epoch 49/140 (lr=0.0001), train loss 0.00179
Epoch 50/140 (lr=0.0001), train loss 0.00160
Epoch 51/140 (lr=0.0001), train loss 0.00162
Epoch 52/140 (lr=0.0001), train loss 0.00150
Epoch 53/140 (lr=0.0001), train loss 0.00156
Epoch 54/140 (lr=0.0001), train loss 0.00152
Epoch 55/140 (lr=0.0001), train loss 0.00147
Epoch 56/140 (lr=0.0001), train loss 0.00158
Epoch 57/140 (lr=0.0001), train loss 0.00148
Epoch 58/140 (lr=0.0001), train loss 0.00142
Epoch 59/140 (lr=0.0001), train loss 0.00148
Epoch 60/140 (lr=0.0001), train loss 0.00152
Epoch 61/140 (lr=0.0001), train loss 0.00143
Epoch 62/140 (lr=0.0001), train loss 0.00134
Epoch 63/140 (lr=0.0001), train loss 0.00131
Epoch 64/140 (lr=0.0001), train loss 0.00136
Epoch 65/140 (lr=0.0001), train loss 0.00133
Epoch 66/140 (lr=0.0001), train loss 0.00128
Epoch 67/140 (lr=0.0001), train loss 0.00128
Epoch 68/140 (lr=0.0001), train loss 0.00128
Epoch 69/140 (lr=0.0001), train loss 0.00126
Epoch 70/140 (lr=0.0001), train loss 0.00128
Epoch 71/140 (lr=0.0001), train loss 0.00130
Epoch 72/140 (lr=0.0001), train loss 0.00136
Epoch 73/140 (lr=0.0001), train loss 0.00139
Epoch 74/140 (lr=0.0001), train loss 0.00124
Epoch 75/140 (lr=0.0001), train loss 0.00123
Epoch 76/140 (lr=0.0001), train loss 0.00123
Epoch 77/140 (lr=0.0001), train loss 0.00119
Epoch 78/140 (lr=0.0001), train loss 0.00137
Epoch 79/140 (lr=0.0001), train loss 0.00124
Epoch 80/140 (lr=0.0001), train loss 0.00121
Epoch 81/140 (lr=0.0001), train loss 0.00127
Epoch 82/140 (lr=0.0001), train loss 0.00135
Epoch 83/140 (lr=0.0001), train loss 0.00251
Epoch 84/140 (lr=0.0001), train loss 0.00152
Epoch 85/140 (lr=0.0001), train loss 0.00196
Epoch 86/140 (lr=0.0001), train loss 0.00131
Epoch 87/140 (lr=0.0001), train loss 0.00152
Epoch 88/140 (lr=0.0001), train loss 0.00121
Epoch 89/140 (lr=0.0001), train loss 0.00113
Epoch 90/140 (lr=0.0001), train loss 0.00115
Epoch 91/140 (lr=0.0001), train loss 0.00109
Epoch 92/140 (lr=0.0001), train loss 0.00117
Epoch 93/140 (lr=0.0001), train loss 0.00115
Epoch 94/140 (lr=0.0001), train loss 0.00113
Epoch 95/140 (lr=0.0001), train loss 0.00126
Epoch 96/140 (lr=0.0001), train loss 0.00124
Epoch 97/140 (lr=0.0001), train loss 0.00113
Epoch 98/140 (lr=0.0001), train loss 0.00105
Epoch 99/140 (lr=0.0001), train loss 0.00119
Epoch 100/140 (lr=0.0001), train loss 0.00132
Epoch 101/140 (lr=0.0001), train loss 0.00114
Epoch 102/140 (lr=0.0001), train loss 0.00107
Epoch 103/140 (lr=0.0001), train loss 0.00100
Epoch 104/140 (lr=0.0001), train loss 0.00103
Epoch 105/140 (lr=0.0001), train loss 0.00102
Epoch 106/140 (lr=0.0001), train loss 0.00098
Epoch 107/140 (lr=0.0001), train loss 0.00105
Epoch 108/140 (lr=0.0001), train loss 0.00101
Epoch 109/140 (lr=0.0001), train loss 0.00098
Epoch 110/140 (lr=0.0001), train loss 0.00100
Epoch 111/140 (lr=0.0001), train loss 0.00099
Epoch 112/140 (lr=0.0001), train loss 0.00097
Epoch 113/140 (lr=0.0001), train loss 0.00096
Epoch 114/140 (lr=0.0001), train loss 0.00096
Epoch 115/140 (lr=0.0001), train loss 0.00096
Epoch 116/140 (lr=0.0001), train loss 0.00093
Epoch 117/140 (lr=0.0001), train loss 0.00094
Epoch 118/140 (lr=0.0001), train loss 0.00100
Epoch 119/140 (lr=0.0001), train loss 0.00093
Epoch 120/140 (lr=0.0001), train loss 0.00088
Epoch 121/140 (lr=0.0001), train loss 0.00112
Epoch 122/140 (lr=0.0001), train loss 0.00097
Epoch 123/140 (lr=0.0001), train loss 0.00139
Epoch 124/140 (lr=0.0001), train loss 0.00103
Epoch 125/140 (lr=0.0001), train loss 0.00091
Epoch 126/140 (lr=0.0001), train loss 0.00087
Epoch 127/140 (lr=0.0001), train loss 0.00089
Epoch 128/140 (lr=0.0001), train loss 0.00087
Epoch 129/140 (lr=0.0001), train loss 0.00086
Epoch 130/140 (lr=0.0001), train loss 0.00085
Epoch 131/140 (lr=0.0001), train loss 0.00083
Epoch 132/140 (lr=0.0001), train loss 0.00084
Epoch 133/140 (lr=0.0001), train loss 0.00085
Epoch 134/140 (lr=0.0001), train loss 0.00082
Epoch 135/140 (lr=0.0001), train loss 0.00084
Epoch 136/140 (lr=0.0001), train loss 0.00085
Epoch 137/140 (lr=0.0001), train loss 0.00086
Epoch 138/140 (lr=0.0001), train loss 0.00082
Epoch 139/140 (lr=0.0001), train loss 0.00079
Epoch 140/140 (lr=0.0001), train loss 0.00088
Loading DLC 3.0.0rc5...
DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)
Updated Configuration:
all_joints:
- - 0
- - 1
- - 2
- - 3
- - 4
- - 5
- - 6
- - 7
- - 8
all_joints_names:
- nose
- lefteye
- righteye
- spine1
- spine2
- spine3
- backfin
- leftfin
- rightfin
alpha_r: 0.02
apply_prob: 0.5
batch_size: 8
contrast:
  clahe: true
  claheratio: 0.1
  histeq: true
  histeqratio: 0.1
convolution:
  edge: false
  emboss:
    alpha:
    - 0.0
    - 1.0
    strength:
    - 0.5
    - 1.5
  embossratio: 0.1
  sharpen: false
  sharpenratio: 0.3
crop_sampling: hybrid
crop_size:
- 400
- 400
cropratio: 0.4
dataset: training-datasets/iteration-0/UnaugmentedDataSet_dlc_modelJul26/dlc_model_student95shuffle1.pickle
dataset_type: multi-animal-imgaug
decay_steps: 30000
display_iters: 500
engine: pytorch
epochs: 115
eval_interval: 300
global_scale: 0.8
init_weights: /home/hice1/kcozart6/.conda/envs/DEEPLABCUT/lib/python3.10/site-packages/deeplabcut
intermediate_supervision: false
intermediate_supervision_layer: 12
location_refinement: true
locref_huber_loss: true
locref_loss_weight: 0.05
locref_stdev: 7.2801
lr_init: 0.0005
max_input_size: 1500
max_shift: 0.4
maxiters: 20000
metadataset: training-datasets/iteration-0/UnaugmentedDataSet_dlc_modelJul26/Documentation_data-dlc_model_95shuffle1.pickle
min_input_size: 64
mirror: false
multi_stage: false
multi_step:
- - 0.0001
  - 7500
- - 5.0e-05
  - 12000
- - 1.0e-05
  - 200000
net_type: resnet_50
num_idchannel: 0
num_joints: 9
num_limbs: 36
optimizer: adam
pafwidth: 20
pairwise_huber_loss: false
pairwise_loss_weight: 0.1
pairwise_predict: false
partaffinityfield_graph:
- - 0
  - 1
- - 0
  - 2
- - 0
  - 3
- - 0
  - 4
- - 0
  - 5
- - 0
  - 6
- - 0
  - 7
- - 0
  - 8
- - 1
  - 2
- - 1
  - 3
- - 1
  - 4
- - 1
  - 5
- - 1
  - 6
- - 1
  - 7
- - 1
  - 8
- - 2
  - 3
- - 2
  - 4
- - 2
  - 5
- - 2
  - 6
- - 2
  - 7
- - 2
  - 8
- - 3
  - 4
- - 3
  - 5
- - 3
  - 6
- - 3
  - 7
- - 3
  - 8
- - 4
  - 5
- - 4
  - 6
- - 4
  - 7
- - 4
  - 8
- - 5
  - 6
- - 5
  - 7
- - 5
  - 8
- - 6
  - 7
- - 6
  - 8
- - 7
  - 8
partaffinityfield_predict: true
pos_dist_thresh: 17
pre_resize: []
project_path: /storage/ice1/0/3/kcozart6/dlc_model-student-2023-07-26
rotation: 25
rotratio: 0.4
save_iters: 15000
scale_jitter_lo: 0.5
scale_jitter_up: 1.25
weigh_only_present_joints: false

Utilizing the following graph: [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 5], [4, 6], [4, 7], [4, 8], [5, 6], [5, 7], [5, 8], [6, 7], [6, 8], [7, 8]]
Creating training data for: Shuffle: 2 TrainFraction:  0.95
  0%|          | 0/1472 [00:00<?, ?it/s]  3%|▎         | 51/1472 [00:00<00:04, 320.82it/s]  6%|▌         | 84/1472 [00:00<00:04, 281.54it/s] 11%|█         | 162/1472 [00:00<00:02, 442.11it/s] 14%|█▍        | 209/1472 [00:00<00:03, 404.52it/s] 17%|█▋        | 255/1472 [00:00<00:02, 414.72it/s] 20%|██        | 298/1472 [00:00<00:03, 382.29it/s] 25%|██▍       | 364/1472 [00:00<00:02, 440.40it/s] 28%|██▊       | 409/1472 [00:01<00:02, 421.99it/s] 33%|███▎      | 487/1472 [00:01<00:01, 494.22it/s] 36%|███▋      | 537/1472 [00:01<00:02, 444.09it/s] 41%|████      | 603/1472 [00:01<00:01, 454.85it/s] 48%|████▊     | 701/1472 [00:01<00:01, 533.68it/s] 53%|█████▎    | 780/1472 [00:01<00:01, 579.42it/s] 57%|█████▋    | 839/1472 [00:01<00:01, 515.91it/s] 61%|██████    | 892/1472 [00:01<00:01, 452.21it/s] 64%|██████▍   | 941/1472 [00:02<00:01, 460.89it/s] 67%|██████▋   | 989/1472 [00:02<00:01, 464.58it/s] 71%|███████▏  | 1049/1472 [00:02<00:00, 463.97it/s] 75%|███████▍  | 1102/1472 [00:02<00:00, 452.49it/s] 78%|███████▊  | 1148/1472 [00:02<00:00, 406.43it/s] 81%|████████  | 1190/1472 [00:02<00:00, 403.87it/s] 84%|████████▍ | 1240/1472 [00:02<00:00, 406.34it/s] 87%|████████▋ | 1282/1472 [00:02<00:00, 395.35it/s] 92%|█████████▏| 1361/1472 [00:03<00:00, 498.57it/s] 96%|█████████▌| 1413/1472 [00:03<00:00, 432.00it/s] 99%|█████████▉| 1462/1472 [00:03<00:00, 438.51it/s]100%|██████████| 1472/1472 [00:03<00:00, 446.67it/s]
Traceback (most recent call last):
  File "/home/hice1/kcozart6/scratch/run_model.py", line 61, in <module>
    deeplabcut.create_multianimaltraining_dataset(config_path, trainIndices=None)
  File "/home/hice1/kcozart6/.conda/envs/DEEPLABCUT/lib/python3.10/site-packages/deeplabcut/generate_training_dataset/multiple_individuals_trainingsetmanipulation.py", line 435, in create_multianimaltraining_dataset
    metadata.update_metadata(
  File "/home/hice1/kcozart6/.conda/envs/DEEPLABCUT/lib/python3.10/site-packages/deeplabcut/generate_training_dataset/metadata.py", line 381, in update_metadata
    metadata = metadata.add(shuffle=new_shuffle, overwrite=overwrite)
  File "/home/hice1/kcozart6/.conda/envs/DEEPLABCUT/lib/python3.10/site-packages/deeplabcut/generate_training_dataset/metadata.py", line 169, in add
    raise RuntimeError(
RuntimeError: Cannot add ShuffleMetadata(name='dlc_modelJul26-trainset95shuffle2', train_fraction=0.95, index=2, engine=Engine.PYTORCH, split=DataSplit(train_indices=(0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 164, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 354, 355, 356, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 416, 417, 418, 419, 420, 421, 422, 424, 425, 426, 427, 428, 429, 430, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 494, 495, 496, 499, 500, 501, 502, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 693, 694, 695, 696, 697, 698, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 723, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1159, 1160, 1161, 1162, 1163, 1164, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1267, 1268, 1269, 1270, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1285, 1286, 1287, 1288, 1289, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1385, 1386, 1387, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1548, 1549), test_indices=(1, 19, 39, 46, 57, 72, 82, 98, 141, 145, 162, 163, 165, 195, 244, 249, 263, 265, 275, 276, 300, 320, 334, 353, 357, 372, 415, 423, 431, 443, 476, 477, 493, 497, 498, 503, 546, 601, 646, 690, 691, 692, 699, 711, 721, 722, 724, 738, 752, 766, 837, 859, 877, 952, 988, 1003, 1016, 1030, 1065, 1117, 1136, 1158, 1165, 1192, 1210, 1266, 1271, 1284, 1290, 1384, 1390, 1394, 1417, 1465, 1481, 1509, 1521, 1547))) to the meta: a shuffle with index 2 and train_fraction 0.95 already exists: (ShuffleMetadata(name='dlc_modelJul26-trainset95shuffle1', train_fraction=0.95, index=1, engine=Engine.PYTORCH, split=DataSplit(train_indices=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 92, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 389, 390, 391, 392, 393, 394, 395, 396, 397, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 424, 425, 426, 427, 428, 429, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 538, 539, 540, 541, 542, 545, 546, 547, 548, 549, 550, 552, 553, 554, 555, 556, 557, 558, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 634, 635, 636, 637, 638, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 676, 677, 678, 679, 680, 681, 682, 683, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 698, 699, 700, 701, 702, 703, 704, 706, 707, 708, 709, 710, 711, 712, 713, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 757, 758, 759, 760, 761, 762, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 778, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 798, 799, 800, 801, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 836, 837, 838, 839, 840, 841, 842, 843, 844, 846, 847, 848, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 883, 884, 885, 886, 887, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 909, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1202, 1203, 1204, 1205, 1206, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1507, 1508, 1509, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549), test_indices=(28, 57, 72, 87, 91, 99, 115, 128, 151, 174, 244, 257, 273, 277, 291, 314, 335, 388, 398, 423, 430, 448, 510, 537, 543, 544, 551, 559, 599, 600, 611, 633, 639, 659, 675, 684, 697, 705, 714, 755, 756, 763, 777, 779, 797, 802, 835, 845, 849, 850, 882, 888, 908, 910, 976, 1033, 1066, 1077, 1094, 1106, 1143, 1171, 1201, 1207, 1216, 1289, 1312, 1345, 1383, 1420, 1455, 1466, 1483, 1495, 1496, 1506, 1510, 1523))), ShuffleMetadata(name='dlc_modelJul26-trainset95shuffle2', train_fraction=0.95, index=2, engine=Engine.PYTORCH, split=DataSplit(train_indices=(0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 164, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 354, 355, 356, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 416, 417, 418, 419, 420, 421, 422, 424, 425, 426, 427, 428, 429, 430, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 494, 495, 496, 499, 500, 501, 502, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 693, 694, 695, 696, 697, 698, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 712, 713, 714, 715, 716, 717, 718, 719, 720, 723, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1159, 1160, 1161, 1162, 1163, 1164, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1267, 1268, 1269, 1270, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1285, 1286, 1287, 1288, 1289, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1385, 1386, 1387, 1388, 1389, 1391, 1392, 1393, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1548, 1549), test_indices=(1, 19, 39, 46, 57, 72, 82, 98, 141, 145, 162, 163, 165, 195, 244, 249, 263, 265, 275, 276, 300, 320, 334, 353, 357, 372, 415, 423, 431, 443, 476, 477, 493, 497, 498, 503, 546, 601, 646, 690, 691, 692, 699, 711, 721, 722, 724, 738, 752, 766, 837, 859, 877, 952, 988, 1003, 1016, 1030, 1065, 1117, 1136, 1158, 1165, 1192, 1210, 1266, 1271, 1284, 1290, 1384, 1390, 1394, 1417, 1465, 1481, 1509, 1521, 1547)))).
srun: error: atl1-1-03-013-3-0: task 0: Exited with exit code 1
---------------------------------------
Begin Slurm Epilog: Nov-04-2024 12:37:37
Job ID:        908359
Array Job ID:  _4294967294
User ID:       kcozart6
Account:       coc
Job name:      DLC_model_training
Resources:     cpu=4,gres/gpu:h100=1,mem=64G,node=1
Rsrc Used:     cput=1-20:20:44,vmem=0,walltime=11:05:11,mem=6480K,energy_used=0
Partition:     ice-gpu
Nodes:         atl1-1-03-013-3-0
---------------------------------------
