---------------------------------------
Begin Slurm Prolog: Nov-04-2024 18:56:54
Job ID:    910069
User ID:   kcozart6
Account:   coc
Job name:  DLC_model_training
Partition: ice-gpu
---------------------------------------

CondaError: Run 'conda init' before 'conda activate'

Training with configuration:
data:
  colormode: RGB
  inference:
    normalize_images: True
  train:
    affine:
      p: 0.5
      rotation: 30
      scaling: [1.0, 1.0]
      translation: 0
    collate:
      type: ResizeFromDataSizeCollate
      min_scale: 0.4
      max_scale: 1.0
      min_short_side: 128
      max_short_side: 1152
      multiple_of: 32
      to_square: False
    covering: False
    gaussian_noise: 12.75
    hist_eq: False
    motion_blur: False
    normalize_images: True
device: auto
metadata:
  project_path: /storage/ice1/0/3/kcozart6/dlc_model-student-2023-07-26
  pose_config_path: /storage/ice1/0/3/kcozart6/dlc_model-student-2023-07-26/dlc-models-pytorch/iteration-0/dlc_modelJul26-trainset95shuffle1/train/pose_cfg.yaml
  bodyparts: ['nose', 'lefteye', 'righteye', 'spine1', 'spine2', 'spine3', 'backfin', 'leftfin', 'rightfin']
  unique_bodyparts: []
  individuals: ['fish1', 'fish2', 'fish3', 'fish4', 'fish5', 'fish6', 'fish7', 'fish8', 'fish9', 'fish10']
  with_identity: False
method: bu
model:
  backbone:
    type: ResNet
    model_name: resnet50_gn
    output_stride: 16
    freeze_bn_stats: True
    freeze_bn_weights: False
  backbone_output_channels: 2048
  heads:
    bodypart:
      type: DLCRNetHead
      predictor:
        type: PartAffinityFieldPredictor
        num_animals: 10
        num_multibodyparts: 9
        num_uniquebodyparts: 0
        nms_radius: 5
        sigma: 1.0
        locref_stdev: 7.2801
        min_affinity: 0.05
        graph: [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 5], [4, 6], [4, 7], [4, 8], [5, 6], [5, 7], [5, 8], [6, 7], [6, 8], [7, 8]]
        edges_to_keep: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
        apply_sigmoid: True
        clip_scores: False
      target_generator:
        type: SequentialGenerator
        generators: [{'type': 'HeatmapPlateauGenerator', 'num_heatmaps': 9, 'pos_dist_thresh': 17, 'heatmap_mode': 'KEYPOINT', 'gradient_masking': False, 'generate_locref': True, 'locref_std': 7.2801}, {'type': 'PartAffinityFieldGenerator', 'graph': [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 5], [4, 6], [4, 7], [4, 8], [5, 6], [5, 7], [5, 8], [6, 7], [6, 8], [7, 8]], 'width': 20}]
      criterion:
        heatmap:
          type: WeightedBCECriterion
          weight: 1.0
        locref:
          type: WeightedHuberCriterion
          weight: 0.05
        paf:
          type: WeightedHuberCriterion
          weight: 0.1
      heatmap_config:
        channels: [2048, 9]
        kernel_size: [3]
        strides: [2]
      locref_config:
        channels: [2048, 18]
        kernel_size: [3]
        strides: [2]
      paf_config:
        channels: [2048, 72]
        kernel_size: [3]
        strides: [2]
      num_stages: 5
net_type: resnet_50
runner:
  type: PoseTrainingRunner
  gpus: None
  key_metric: test.mAP
  key_metric_asc: True
  eval_interval: 300
  optimizer:
    type: AdamW
    params:
      lr: 0.0001
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [[1e-05], [1e-06]]
      milestones: [160, 190]
  snapshots:
    max_snapshots: 6
    save_epochs: 25
    save_optimizer_state: False
train_settings:
  batch_size: 4
  dataloader_workers: 0
  dataloader_pin_memory: False
  display_iters: 1000
  epochs: 140
  seed: 42
Loading pretrained weights from Hugging Face hub (timm/resnet50_gn.a1h_in1k)
[timm/resnet50_gn.a1h_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
Data Transforms:
  Training:   Compose([
  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (1.0, 1.0), 'y': (1.0, 1.0)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),
  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
  Validation: Compose([
  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),
], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)
Using custom collate function: {'type': 'ResizeFromDataSizeCollate', 'min_scale': 0.4, 'max_scale': 1.0, 'min_short_side': 128, 'max_short_side': 1152, 'multiple_of': 32, 'to_square': False}

Note: According to your model configuration, you're training with batch size 1 and/or ``freeze_bn_stats=false``. This is not an optimal setting if you have powerful GPUs.
This is good for small batch sizes (e.g., when training on a CPU), where you should keep ``freeze_bn_stats=true``.
If you're using a GPU to train, you can obtain faster performance by setting a larger batch size (the biggest power of 2 where you don't geta CUDA out-of-memory error, such as 8, 16, 32 or 64 depending on the model, size of your images, and GPU memory) and ``freeze_bn_stats=false`` for the backbone of your model. 
This also allows you to increase the learning rate (empirically you can scale the learning rate by sqrt(batch_size) times).

Using 1472 images and 78 for testing

Starting pose model training...
--------------------------------------------------
Epoch 1/140 (lr=0.0001), train loss 0.00961
Epoch 2/140 (lr=0.0001), train loss 0.00545
Epoch 3/140 (lr=0.0001), train loss 0.00397
Epoch 4/140 (lr=0.0001), train loss 0.00422
Epoch 5/140 (lr=0.0001), train loss 0.00418
Epoch 6/140 (lr=0.0001), train loss 0.00353
Epoch 7/140 (lr=0.0001), train loss 0.00384
Epoch 8/140 (lr=0.0001), train loss 0.00451
Epoch 9/140 (lr=0.0001), train loss 0.00475
Epoch 10/140 (lr=0.0001), train loss 0.00284
Epoch 11/140 (lr=0.0001), train loss 0.00452
Epoch 12/140 (lr=0.0001), train loss 0.00293
Epoch 13/140 (lr=0.0001), train loss 0.00307
Epoch 14/140 (lr=0.0001), train loss 0.00242
Epoch 15/140 (lr=0.0001), train loss 0.00229
Epoch 16/140 (lr=0.0001), train loss 0.00247
Epoch 17/140 (lr=0.0001), train loss 0.00284
Epoch 18/140 (lr=0.0001), train loss 0.00280
Epoch 19/140 (lr=0.0001), train loss 0.00250
Epoch 20/140 (lr=0.0001), train loss 0.00342
Epoch 21/140 (lr=0.0001), train loss 0.00282
Epoch 22/140 (lr=0.0001), train loss 0.00219
Epoch 23/140 (lr=0.0001), train loss 0.00212
Epoch 24/140 (lr=0.0001), train loss 0.00229
Epoch 25/140 (lr=0.0001), train loss 0.00219
Epoch 26/140 (lr=0.0001), train loss 0.00208
Epoch 27/140 (lr=0.0001), train loss 0.00236
Epoch 28/140 (lr=0.0001), train loss 0.00204
Epoch 29/140 (lr=0.0001), train loss 0.00193
Epoch 30/140 (lr=0.0001), train loss 0.00195
Epoch 31/140 (lr=0.0001), train loss 0.00195
Epoch 32/140 (lr=0.0001), train loss 0.00204
Epoch 33/140 (lr=0.0001), train loss 0.00196
Epoch 34/140 (lr=0.0001), train loss 0.00185
Epoch 35/140 (lr=0.0001), train loss 0.00179
Epoch 36/140 (lr=0.0001), train loss 0.00183
Epoch 37/140 (lr=0.0001), train loss 0.00186
Epoch 38/140 (lr=0.0001), train loss 0.00167
Epoch 39/140 (lr=0.0001), train loss 0.00200
Epoch 40/140 (lr=0.0001), train loss 0.00164
Epoch 41/140 (lr=0.0001), train loss 0.00176
Epoch 42/140 (lr=0.0001), train loss 0.00159
Epoch 43/140 (lr=0.0001), train loss 0.00232
Epoch 44/140 (lr=0.0001), train loss 0.00171
Epoch 45/140 (lr=0.0001), train loss 0.00166
Epoch 46/140 (lr=0.0001), train loss 0.00164
Epoch 47/140 (lr=0.0001), train loss 0.00157
Epoch 48/140 (lr=0.0001), train loss 0.00159
Epoch 49/140 (lr=0.0001), train loss 0.00179
Epoch 50/140 (lr=0.0001), train loss 0.00160
Epoch 51/140 (lr=0.0001), train loss 0.00162
Epoch 52/140 (lr=0.0001), train loss 0.00150
Epoch 53/140 (lr=0.0001), train loss 0.00156
Epoch 54/140 (lr=0.0001), train loss 0.00152
Epoch 55/140 (lr=0.0001), train loss 0.00147
Epoch 56/140 (lr=0.0001), train loss 0.00158
Epoch 57/140 (lr=0.0001), train loss 0.00148
Epoch 58/140 (lr=0.0001), train loss 0.00142
Epoch 59/140 (lr=0.0001), train loss 0.00148
Epoch 60/140 (lr=0.0001), train loss 0.00152
Epoch 61/140 (lr=0.0001), train loss 0.00143
Epoch 62/140 (lr=0.0001), train loss 0.00134
Epoch 63/140 (lr=0.0001), train loss 0.00131
Epoch 64/140 (lr=0.0001), train loss 0.00136
Epoch 65/140 (lr=0.0001), train loss 0.00133
Epoch 66/140 (lr=0.0001), train loss 0.00128
Epoch 67/140 (lr=0.0001), train loss 0.00128
Epoch 68/140 (lr=0.0001), train loss 0.00128
Epoch 69/140 (lr=0.0001), train loss 0.00126
Epoch 70/140 (lr=0.0001), train loss 0.00128
Epoch 71/140 (lr=0.0001), train loss 0.00130
Epoch 72/140 (lr=0.0001), train loss 0.00136
Epoch 73/140 (lr=0.0001), train loss 0.00139
Epoch 74/140 (lr=0.0001), train loss 0.00124
Epoch 75/140 (lr=0.0001), train loss 0.00123
Epoch 76/140 (lr=0.0001), train loss 0.00123
Epoch 77/140 (lr=0.0001), train loss 0.00119
Epoch 78/140 (lr=0.0001), train loss 0.00137
Epoch 79/140 (lr=0.0001), train loss 0.00124
Epoch 80/140 (lr=0.0001), train loss 0.00121
Epoch 81/140 (lr=0.0001), train loss 0.00127
Epoch 82/140 (lr=0.0001), train loss 0.00135
Epoch 83/140 (lr=0.0001), train loss 0.00251
Epoch 84/140 (lr=0.0001), train loss 0.00152
Epoch 85/140 (lr=0.0001), train loss 0.00196
Epoch 86/140 (lr=0.0001), train loss 0.00131
Epoch 87/140 (lr=0.0001), train loss 0.00152
Epoch 88/140 (lr=0.0001), train loss 0.00121
Epoch 89/140 (lr=0.0001), train loss 0.00113
Epoch 90/140 (lr=0.0001), train loss 0.00115
Epoch 91/140 (lr=0.0001), train loss 0.00109
Epoch 92/140 (lr=0.0001), train loss 0.00117
Epoch 93/140 (lr=0.0001), train loss 0.00115
Epoch 94/140 (lr=0.0001), train loss 0.00113
Epoch 95/140 (lr=0.0001), train loss 0.00126
Epoch 96/140 (lr=0.0001), train loss 0.00124
Epoch 97/140 (lr=0.0001), train loss 0.00113
Epoch 98/140 (lr=0.0001), train loss 0.00105
Epoch 99/140 (lr=0.0001), train loss 0.00119
Epoch 100/140 (lr=0.0001), train loss 0.00132
Epoch 101/140 (lr=0.0001), train loss 0.00114
Epoch 102/140 (lr=0.0001), train loss 0.00107
Epoch 103/140 (lr=0.0001), train loss 0.00100
Epoch 104/140 (lr=0.0001), train loss 0.00103
Epoch 105/140 (lr=0.0001), train loss 0.00102
Epoch 106/140 (lr=0.0001), train loss 0.00098
Epoch 107/140 (lr=0.0001), train loss 0.00105
Epoch 108/140 (lr=0.0001), train loss 0.00101
Epoch 109/140 (lr=0.0001), train loss 0.00098
Epoch 110/140 (lr=0.0001), train loss 0.00100
Epoch 111/140 (lr=0.0001), train loss 0.00099
Epoch 112/140 (lr=0.0001), train loss 0.00097
Epoch 113/140 (lr=0.0001), train loss 0.00096
Epoch 114/140 (lr=0.0001), train loss 0.00096
Epoch 115/140 (lr=0.0001), train loss 0.00096
Epoch 116/140 (lr=0.0001), train loss 0.00093
Epoch 117/140 (lr=0.0001), train loss 0.00094
Epoch 118/140 (lr=0.0001), train loss 0.00100
Epoch 119/140 (lr=0.0001), train loss 0.00093
Epoch 120/140 (lr=0.0001), train loss 0.00088
Epoch 121/140 (lr=0.0001), train loss 0.00112
Epoch 122/140 (lr=0.0001), train loss 0.00097
Epoch 123/140 (lr=0.0001), train loss 0.00139
Epoch 124/140 (lr=0.0001), train loss 0.00103
Epoch 125/140 (lr=0.0001), train loss 0.00091
Epoch 126/140 (lr=0.0001), train loss 0.00087
Epoch 127/140 (lr=0.0001), train loss 0.00089
Epoch 128/140 (lr=0.0001), train loss 0.00087
Epoch 129/140 (lr=0.0001), train loss 0.00086
Epoch 130/140 (lr=0.0001), train loss 0.00085
Epoch 131/140 (lr=0.0001), train loss 0.00083
Epoch 132/140 (lr=0.0001), train loss 0.00084
Epoch 133/140 (lr=0.0001), train loss 0.00085
Epoch 134/140 (lr=0.0001), train loss 0.00082
Epoch 135/140 (lr=0.0001), train loss 0.00084
Epoch 136/140 (lr=0.0001), train loss 0.00085
Epoch 137/140 (lr=0.0001), train loss 0.00086
Epoch 138/140 (lr=0.0001), train loss 0.00082
Epoch 139/140 (lr=0.0001), train loss 0.00079
Epoch 140/140 (lr=0.0001), train loss 0.00088
Loading DLC 3.0.0rc5...
DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)
Updated Configuration:
all_joints:
- - 0
- - 1
- - 2
- - 3
- - 4
- - 5
- - 6
- - 7
- - 8
all_joints_names:
- nose
- lefteye
- righteye
- spine1
- spine2
- spine3
- backfin
- leftfin
- rightfin
alpha_r: 0.02
apply_prob: 0.5
batch_size: 8
contrast:
  clahe: true
  claheratio: 0.1
  histeq: true
  histeqratio: 0.1
convolution:
  edge: false
  emboss:
    alpha:
    - 0.0
    - 1.0
    strength:
    - 0.5
    - 1.5
  embossratio: 0.1
  sharpen: false
  sharpenratio: 0.3
crop_sampling: hybrid
crop_size:
- 400
- 400
cropratio: 0.4
dataset: training-datasets/iteration-0/UnaugmentedDataSet_dlc_modelJul26/dlc_model_student95shuffle1.pickle
dataset_type: multi-animal-imgaug
decay_steps: 30000
display_iters: 500
engine: pytorch
global_scale: 0.8
init_weights: /home/hice1/kcozart6/.conda/envs/DEEPLABCUT/lib/python3.10/site-packages/deeplabcut
intermediate_supervision: false
intermediate_supervision_layer: 12
location_refinement: true
locref_huber_loss: true
locref_loss_weight: 0.05
locref_stdev: 7.2801
lr_init: 0.0005
max_input_size: 1500
max_shift: 0.4
metadataset: training-datasets/iteration-0/UnaugmentedDataSet_dlc_modelJul26/Documentation_data-dlc_model_95shuffle1.pickle
min_input_size: 64
mirror: false
multi_stage: false
multi_step:
- - 0.0001
  - 7500
- - 5.0e-05
  - 12000
- - 1.0e-05
  - 200000
net_type: resnet_50
num_idchannel: 0
num_joints: 9
num_limbs: 36
optimizer: adam
pafwidth: 20
pairwise_huber_loss: false
pairwise_loss_weight: 0.1
pairwise_predict: false
partaffinityfield_graph:
- - 0
  - 1
- - 0
  - 2
- - 0
  - 3
- - 0
  - 4
- - 0
  - 5
- - 0
  - 6
- - 0
  - 7
- - 0
  - 8
- - 1
  - 2
- - 1
  - 3
- - 1
  - 4
- - 1
  - 5
- - 1
  - 6
- - 1
  - 7
- - 1
  - 8
- - 2
  - 3
- - 2
  - 4
- - 2
  - 5
- - 2
  - 6
- - 2
  - 7
- - 2
  - 8
- - 3
  - 4
- - 3
  - 5
- - 3
  - 6
- - 3
  - 7
- - 3
  - 8
- - 4
  - 5
- - 4
  - 6
- - 4
  - 7
- - 4
  - 8
- - 5
  - 6
- - 5
  - 7
- - 5
  - 8
- - 6
  - 7
- - 6
  - 8
- - 7
  - 8
partaffinityfield_predict: true
pos_dist_thresh: 17
pre_resize: []
project_path: /storage/ice1/0/3/kcozart6/dlc_model-student-2023-07-26
rotation: 25
rotratio: 0.4
save_iters: 10000
scale_jitter_lo: 0.5
scale_jitter_up: 1.25
weigh_only_present_joints: false

Utilizing the following graph: [[0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 4], [3, 5], [3, 6], [3, 7], [3, 8], [4, 5], [4, 6], [4, 7], [4, 8], [5, 6], [5, 7], [5, 8], [6, 7], [6, 8], [7, 8]]
Creating training data for: Shuffle: 2 TrainFraction:  0.95
  0%|          | 0/1472 [00:00<?, ?it/s]  3%|▎         | 51/1472 [00:00<00:03, 402.49it/s]  6%|▋         | 92/1472 [00:00<00:03, 351.09it/s] 10%|█         | 152/1472 [00:00<00:02, 449.24it/s] 14%|█▍        | 203/1472 [00:00<00:02, 439.50it/s] 17%|█▋        | 255/1472 [00:00<00:02, 460.62it/s] 21%|██        | 302/1472 [00:00<00:02, 437.60it/s] 25%|██▍       | 364/1472 [00:00<00:02, 463.15it/s] 28%|██▊       | 411/1472 [00:00<00:02, 431.08it/s] 33%|███▎      | 487/1472 [00:01<00:02, 477.23it/s] 36%|███▋      | 535/1472 [00:01<00:02, 453.74it/s] 41%|████      | 603/1472 [00:01<00:01, 473.66it/s] 48%|████▊     | 701/1472 [00:01<00:01, 554.41it/s] 53%|█████▎    | 780/1472 [00:01<00:01, 591.48it/s] 57%|█████▋    | 840/1472 [00:01<00:01, 495.41it/s] 61%|██████▏   | 905/1472 [00:01<00:01, 531.60it/s] 65%|██████▌   | 961/1472 [00:01<00:01, 492.77it/s] 70%|██████▉   | 1026/1472 [00:02<00:00, 517.83it/s] 73%|███████▎  | 1080/1472 [00:02<00:00, 516.42it/s] 77%|███████▋  | 1133/1472 [00:02<00:00, 451.08it/s] 80%|████████  | 1181/1472 [00:02<00:00, 455.44it/s] 84%|████████▍ | 1240/1472 [00:02<00:00, 471.64it/s] 88%|████████▊ | 1294/1472 [00:02<00:00, 488.57it/s] 91%|█████████▏| 1344/1472 [00:02<00:00, 490.18it/s] 96%|█████████▌| 1411/1472 [00:02<00:00, 539.65it/s]100%|█████████▉| 1466/1472 [00:03<00:00, 456.72it/s]100%|██████████| 1472/1472 [00:03<00:00, 480.65it/s]
/home/hice1/kcozart6/.conda/envs/DEEPLABCUT/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/base.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  snapshot = torch.load(snapshot_path, map_location=device)
The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!
  0%|          | 0/1472 [00:00<?, ?it/s]slurmstepd: error: Detected 1 oom_kill event in StepId=910069.0. Some of the step tasks have been OOM Killed.
srun: error: atl1-1-03-013-8-0: task 0: Out Of Memory
---------------------------------------
Begin Slurm Epilog: Nov-05-2024 06:08:13
Job ID:        910069
Array Job ID:  _4294967294
User ID:       kcozart6
Account:       coc
Job name:      DLC_model_training
Resources:     cpu=4,gres/gpu:h100=1,mem=64G,node=1
Rsrc Used:     cput=1-20:45:16,vmem=0,walltime=11:11:19,mem=6500K,energy_used=0
Partition:     ice-gpu
Nodes:         atl1-1-03-013-8-0
---------------------------------------
